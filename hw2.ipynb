{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import zipfile\n",
    "import os\n",
    "from io import BytesIO\n",
    "\n",
    "def download_and_extract_zip(zip_url, extract_path):\n",
    "    \"\"\"Downloads and extracts a ZIP file from Google Drive to the specified path.\"\"\"\n",
    "\n",
    "    try:\n",
    "        response = requests.get(zip_url)\n",
    "        response.raise_for_status()  # Raise an error for bad responses\n",
    "\n",
    "        with zipfile.ZipFile(BytesIO(response.content)) as zip_file:\n",
    "            for member in zip_file.namelist():\n",
    "                filename = os.path.basename(member)\n",
    "                if not filename:\n",
    "                    continue  # Skip directories\n",
    "\n",
    "                source = zip_file.open(member)\n",
    "                target_path = os.path.join(extract_path, filename)\n",
    "\n",
    "                with open(target_path, \"wb\") as target_file:\n",
    "                    target_file.write(source.read())\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Download error for '{zip_url}': {e}\")\n",
    "    except zipfile.BadZipFile:\n",
    "        print(f\"Invalid ZIP file: '{zip_url}'.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error for '{zip_url}': {e}\")\n",
    "\n",
    "# List of Google Drive file IDs\n",
    "zip_file_ids = [\"1As-67MFNrpim_jGYgSOgT2SF119gP790\", \"1pjaRe_lSIygHyZ8luWplSC6fuwAEmwTd\"]\n",
    "\n",
    "# Get the current working directory\n",
    "script_directory = os.getcwd()\n",
    "\n",
    "# Construct direct download links for the ZIP files\n",
    "zip_urls = [f\"https://drive.google.com/uc?export=download&id={file_id}\" for file_id in zip_file_ids]\n",
    "\n",
    "# Download and extract each ZIP file directly into the current working directory\n",
    "for zip_url in zip_urls:\n",
    "    download_and_extract_zip(zip_url, script_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# Read CSV files\n",
    "production_data = pd.read_csv(\"production.csv\")\n",
    "production_data['date'] = pd.to_datetime(production_data['date'])\n",
    "\n",
    "weather_data = pd.read_csv(\"processed_weather.csv\")\n",
    "weather_data['date'] = pd.to_datetime(weather_data['date'])\n",
    "\n",
    "# Sort the data by date and hour\n",
    "production_data = production_data.sort_values(by=[\"date\", \"hour\"])\n",
    "weather_data = weather_data.sort_values(by=[\"date\", \"hour\", \"lat\", \"lon\"])\n",
    "\n",
    "# Fill missing values in weather_data using the previous day's same hour, lat, and lon values\n",
    "for col in weather_data.columns:\n",
    "    if col in ['date', 'hour', 'lat', 'lon']:\n",
    "        continue\n",
    "    weather_data[col] = weather_data.groupby(['hour', 'lat', 'lon'])[col].transform(lambda x: x.fillna(method='ffill'))\n",
    "\n",
    "# Identify common dates and hours present in both datasets\n",
    "common_dates = set(production_data[\"date\"].unique()) & set(weather_data[\"date\"].unique())\n",
    "common_hours = set(production_data[\"hour\"].unique()) & set(weather_data[\"hour\"].unique())\n",
    "\n",
    "# Save each coordinate's weather data to a separate file\n",
    "for lat in weather_data[\"lat\"].unique():\n",
    "    for lon in weather_data[\"lon\"].unique():\n",
    "        # Filter weather data for the specific coordinate\n",
    "        coord_weather = weather_data[(weather_data[\"lat\"] == lat) & (weather_data[\"lon\"] == lon)]\n",
    "\n",
    "        # Create the file name\n",
    "        dosya_adi = f\"koordinat_{lat}_{lon}.csv\"\n",
    "\n",
    "        # Write headers if the file does not exist, otherwise append data without headers\n",
    "        if not os.path.exists(dosya_adi):\n",
    "            coord_weather.to_csv(dosya_adi, index=False, header=True)  # Write headers for the first time\n",
    "        else:\n",
    "            coord_weather.to_csv(dosya_adi, mode='a', header=False, index=False)  # Do not write headers for subsequent appends\n",
    "\n",
    "        # Open the file and remove lat and lon columns\n",
    "        df = pd.read_csv(dosya_adi)\n",
    "        df = df.drop([\"lat\", \"lon\"], axis=1)\n",
    "        df.to_csv(dosya_adi, index=False)\n",
    "\n",
    "# Perform correlation analysis for each coordinate\n",
    "koordinat_korelasyonlari = {}\n",
    "for dosya_adi in glob.glob(\"koordinat_*.csv\"):\n",
    "    # Read the coordinate's data\n",
    "    koordinat_data = pd.read_csv(dosya_adi)\n",
    "    koordinat_data['date'] = pd.to_datetime(koordinat_data['date'])\n",
    "\n",
    "    # Filter data for common dates and hours\n",
    "    koordinat_data = koordinat_data[(koordinat_data[\"date\"].isin(common_dates)) & (koordinat_data[\"hour\"].isin(common_hours))]\n",
    "\n",
    "    # Merge production data with coordinate data\n",
    "    merged_data = pd.merge(\n",
    "        production_data[(production_data[\"date\"].isin(common_dates)) & (production_data[\"hour\"].isin(common_hours))],\n",
    "        koordinat_data,\n",
    "        on=[\"date\", \"hour\"]\n",
    "    )\n",
    "\n",
    "    # Calculate correlation for each weather variable\n",
    "    correlations = {}\n",
    "    for col in koordinat_data.columns:\n",
    "        if col in ['date', 'hour']:\n",
    "            continue\n",
    "        correlation = merged_data[\"production\"].corr(merged_data[col])\n",
    "        correlations[col] = correlation\n",
    "\n",
    "    # Save correlations for the coordinate\n",
    "    koordinat_korelasyonlari[dosya_adi] = correlations\n",
    "\n",
    "# Calculate influence factors for each variable for each coordinate\n",
    "koordinat_etkileri = {}\n",
    "for koordinat, korelasyonlar in koordinat_korelasyonlari.items():\n",
    "    koordinat_etkileri[koordinat] = {}\n",
    "    for degisken, korelasyon in korelasyonlar.items():\n",
    "        # Influence is calculated as the square of the absolute value of the correlation\n",
    "        etki = abs(korelasyon) ** 2\n",
    "        koordinat_etkileri[koordinat][degisken] = etki\n",
    "\n",
    "# Normalize influence factors for each variable\n",
    "for degisken in koordinat_etkileri[list(koordinat_etkileri.keys())[0]].keys():\n",
    "    etki_toplami = sum(koordinat_etkileri[koordinat][degisken] for koordinat in koordinat_etkileri)\n",
    "    for koordinat in koordinat_etkileri:\n",
    "        koordinat_etkileri[koordinat][degisken] /= etki_toplami\n",
    "\n",
    "# Convert influence factors to a DataFrame and print them\n",
    "etki_df = pd.DataFrame(koordinat_etkileri).T.reset_index()\n",
    "print(\"Koordinat Etki Çarpanları:\")\n",
    "print(etki_df.to_string(index=False))\n",
    "\n",
    "# Multiply each column in coordinate files by the corresponding influence factor and combine the results\n",
    "sonuc_df = pd.DataFrame()\n",
    "for dosya_adi in glob.glob(\"koordinat_*.csv\"):\n",
    "    # Read the coordinate data\n",
    "    koordinat_data = pd.read_csv(dosya_adi)\n",
    "    koordinat_data['date'] = pd.to_datetime(koordinat_data['date'])\n",
    "\n",
    "    # Get the influence factors for the coordinate\n",
    "    etkiler = koordinat_etkileri[dosya_adi]\n",
    "\n",
    "    # Multiply each column by the corresponding influence factor\n",
    "    for degisken, etki in etkiler.items():\n",
    "        koordinat_data[degisken] = koordinat_data[degisken] * etki\n",
    "\n",
    "    # Group by date and hour and sum the values\n",
    "    koordinat_data = koordinat_data.groupby(['date', 'hour']).sum().reset_index()\n",
    "\n",
    "    # Append to the result DataFrame\n",
    "    sonuc_df = pd.concat([sonuc_df, koordinat_data], ignore_index=True)\n",
    "\n",
    "# Group the results by date and hour and sum the values\n",
    "sonuc_df = sonuc_df.groupby(['date', 'hour']).sum().reset_index()\n",
    "\n",
    "# Round values to 3 decimal places where necessary\n",
    "for col in sonuc_df.columns:\n",
    "    if col in ['date', 'hour']:\n",
    "        continue\n",
    "    sonuc_df[col] = sonuc_df[col].round(3)\n",
    "\n",
    "# Save the final result to a CSV file\n",
    "sonuc_df.to_csv(\"agirlikli.csv\", index=False)\n",
    "\n",
    "# Remove the coordinate files as they are no longer needed\n",
    "for dosya_adi in glob.glob(\"koordinat_*.csv\"):\n",
    "    os.remove(dosya_adi)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
